{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef5eeefc-a05b-4c8b-a19a-bdedd6e2ff70",
   "metadata": {},
   "source": [
    "# 第３章\n",
    "\n",
    "前章から引き続き単語の分散表現について学ぶ。<br>\n",
    "前章では**カウントベース**によって単語の分散表現を得た。<br>\n",
    "本章ではその代わりとして**推論ベースの手法**について学ぶ。<br>\n",
    "\n",
    "**推論ベースの手法**：名前のとおり推論をする手法でニューラルネットワークが使える。<br>\n",
    "                    ここでは**word2vec**について実装し理解を深める。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c75b665-467f-42d1-84fc-742d544b24c1",
   "metadata": {},
   "source": [
    "## 3.1 推論ベースの手法とニューラルネットワーク\n",
    "\n",
    "単語をベクトルで表す手法はこれまで盛んに行われ、その中でも成功した手法は**カウントベースの手法**と**推論ベースの手法**。\n",
    "両者はアプローチの仕方が異なるが背景にはともに**分布仮説**がある。<br>\n",
    "\n",
    "ここではカウントベース手法の問題点を指摘しそれに代わる推論ベース手法の利点を説明する。<br>\n",
    "その後、word2vecの下準備をするために、ニューラルネットワークで「単語」を処理する例をみる。\n",
    "\n",
    "## 3.1.1 カウントベースの手法の問題点\n",
    "\n",
    "＜復習＞\n",
    "単語の共起行列を作りその行列に対してSVDを適用することで単語の分散表現を獲得するもの。\n",
    "\n",
    "**カウントベース手法の問題点**：大規模なコーパスを扱う時に巨大な行列を作ることになり、これに対してSVDを行うことは現実的ではない。（n*n行列に対しn^3の大きさに比例した計算時間がかかる）\n",
    "\n",
    "- カウントベースの手法：コーパス全体の統計データを利用して１回の処理で単語の分散表現を獲得\n",
    "- 推論ベースの手法：ニューラルネットワークを用いる場合はミニバッチで学習\n",
    "\n",
    "一度に少量の学習サンプルを見ながら、重みを繰り返し更新する。枠組みの違いは以下の通り"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb05d27-5d92-465c-8afe-749db85ebbc6",
   "metadata": {},
   "source": [
    "<img src=\"zerotuku2-3-1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7f20a6-2d13-4420-bb53-0d2c4804fee4",
   "metadata": {},
   "source": [
    "ここで推論ベースの強い点は計算量が膨大で処理が困難な場合でも<br>\n",
    "データを小分けにして学習ができる点。<br>\n",
    "また、ニューラルネットワークでは複数のマシン・GPUによる並列計算が可能で<br>\n",
    "全体の学習を高速化できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a068f0a-1489-4291-9924-9ddf91283748",
   "metadata": {},
   "source": [
    "## 3.1.2 推論ベースの手法の概要\n",
    "\n",
    "推論ベースの手法では「推論」が主な作業になる。周囲の単語が与えられたときにどのような単語が得られるかを推測する。<br>\n",
    "このときモデル視点にたつとこの推論問題は以下のようにみえる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a67080-5199-4444-a6a9-6920124be9a4",
   "metadata": {},
   "source": [
    "<img src=\"zerotuku2-3-3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c2e64c-f3f2-49b8-a8e9-b5f4747d9dd7",
   "metadata": {},
   "source": [
    "この図からわかるとおり推論ベースの問題では何らかのモデルが登場する。<br>\n",
    "私達はそのモデルにニューラルネットワークを使う。<br>\n",
    "モデルにはコンテキスト情報を入力として受取り、各単語の出現する確率を出力する。<br>\n",
    "その枠組みの中で正しい推測ができるようにコーパスを使ってモデルの学習を行う。<br>\n",
    "その結果として、単語の分散表現を得るということが<br>\n",
    "推論ベースの手法の全体図となる。\n",
    "\n",
    "## 3.1.3 ニューラルネットワークの単語の処理方法\n",
    "\n",
    "これからニューラルネットワークを使って「単語」を処理する。<br>\n",
    "ニューラルネットワークは単語をそのまま処理できないのでベクトルに変換する。\n",
    "手法の一つとして**one-hot表現**がある。<br>\n",
    "これはベクトルの要素の中で一つだけが１で残りはすべて０であるようなベクトルを意味する。\n",
    "\n",
    "たとえば「You say goodbye and I say hello.」に対して各単語を変換すると<br>\n",
    "you　→　(1,0,0,0,0,0,0)<br>\n",
    "goodbye　→　(0,0,1,0,0,0,0)<br>\n",
    "このように単語を固定長のベクトルにすればニューラルネットワークの入力層は<br>\n",
    "以下のように固定することができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84872169-d958-44ce-b377-70335af6d71b",
   "metadata": {},
   "source": [
    "<img src=\"zerotuku2-3-5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774f0705-927b-414b-a22c-b98fa26eba0e",
   "metadata": {},
   "source": [
    "此の操作によって単語をベクトルで表せるようになったので<br>\n",
    "そのベクトルはニューラルネットワークを構成する<br>\n",
    "様々な「レイヤ」によって処理することが可能になる。\n",
    "\n",
    "全結合層はすべてのノードに矢印によるつながりがある。<br>\n",
    "此の矢印にはすべて重みがあり入力層のニューロンとの重み付き和が<br>\n",
    "中間層のニューロンとなる。<br>\n",
    "本章ではバイアスに関しては省略。\n",
    "\n",
    "重みを明確に示すため、以下のような記法を用いる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca59995-4702-405b-95cd-65f185ebd59f",
   "metadata": {},
   "source": [
    "<img src=\"zerotuku2-3-7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ae0468-9590-4d78-b37c-af68539b6b6e",
   "metadata": {},
   "source": [
    "これまでの話をコードベースで見る。<br>\n",
    "入力層から中間層への変換は以下の通り<br>\n",
    "\n",
    "今回はできる限りtensorflowを導入する。<br>\n",
    "tensorflow：Googleが開発したオープンソースの機械学習（ML）/深層学習（DL）ライブラリ <br>\n",
    "\n",
    "メリット\n",
    "- 自動微分で勾配計算が可能\n",
    "- 学習済みのモデルを再利用可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5802a9f2-e59e-4ad9-8e0f-5b74c1981722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.36355711 0.96910709 0.55053795]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "c=np.array([[1,0,0,0,0,0,0]])#ここでは'you'に対応\n",
    "W=np.random.randn(7,3)#全結合層\n",
    "h=np.dot(c,W)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c626f1e-9f2d-4bf5-91ae-498b36db2244",
   "metadata": {},
   "source": [
    "tensorflowではdotが定義されていないため注意<br>\n",
    "ここでは行列の積としてmatmulを利用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83c974ec-5803-4594-a017-0a32cf1b7df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 01:20:14.258351: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.0934327  -0.66626     0.21555054]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# TensorFlowのTensorで置き換え\n",
    "c = tf.constant([[1, 0, 0, 0, 0, 0, 0]], dtype=tf.float32)\n",
    "W = tf.random.normal((7, 3))\n",
    "h = tf.matmul(c, W)#今回はベクトルの積のためdotと同じ\n",
    "print(h.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e599f8d-d819-472a-939e-2d384371c306",
   "metadata": {},
   "source": [
    "tensorflowでなくても第一章でしたMatMulレイヤで行うことも可能<br>\n",
    "実装すると以下の通り"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "649223c6-8710-4e92-86b5-2bdb61992999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1308704  -1.01777596 -0.76245256]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from layers import MatMul\n",
    "\n",
    "c=np.array([[1,0,0,0,0,0,0]])\n",
    "W=np.random.randn(7,3)\n",
    "layer=MatMul(W)\n",
    "h=layer.forward(c)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01c428f4-cd63-44d9-90b3-81eb4324fef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.5618721 -1.6692069 -1.2896522]]\n"
     ]
    }
   ],
   "source": [
    "#tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "class MatMulTF:\n",
    "    def __init__(self, W):\n",
    "        self.W = tf.Variable(W, dtype=tf.float32)  # 学習するなら Variable（勾配をとって更新）\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 入力 x も Tensor にしておく\n",
    "        return tf.matmul(x, self.W)\n",
    "\n",
    "# One-hot 入力\n",
    "c = tf.constant([[1, 0, 0, 0, 0, 0, 0]], dtype=tf.float32)\n",
    "# 重み\n",
    "W = tf.random.normal((7, 3))\n",
    "# レイヤー\n",
    "layer = MatMulTF(W)\n",
    "# フォワード\n",
    "h = layer.forward(c)\n",
    "print(h.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2769c25b-07b3-44e8-bd3d-53cabebd6f9d",
   "metadata": {},
   "source": [
    "# 3.2 シンプルなword2vec\n",
    "\n",
    "推論ベースの手法を学び、ニューラルネットワークにおける単語の処理方法について<br>\n",
    "コードベースでみた。<br>\n",
    "ここからword2vecの実装に取り掛かる。<br>\n",
    "これからは「モデル」にニューラルネットワーク組み込む。<br>\n",
    "ここでは、**continuous baf-of-words**と呼ばれるモデルを使う。<br>\n",
    "\n",
    "## 3.2.1 CBOWモデルの推論処理\n",
    "\n",
    "CBOWモデル：コンテキストからターゲットを推測することを目的としたニューラルネットワーク。<br>\n",
    "このモデルをできるだけ正確な推測ができるように訓練することで<br>\n",
    "単語の分散表現を獲得することができる。<br>\n",
    "\n",
    "CBOWモデルへの出力はコンテキストで、単語のリストとして表される。<br>\n",
    "これをone-hot表現に変換することでCBOWモデルが処理できるように調整する。<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45646ed1-7413-4bfd-92af-9b97af4b73fa",
   "metadata": {},
   "source": [
    "<img src=\"zerotuku2-3-9.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88337998-0981-4419-8c54-a8584349504d",
   "metadata": {},
   "source": [
    "上の図がCBOWモデルのネットワーク。入力層が２つあり、中間層を経て出力層に行く。<br>\n",
    "入力層から中間層までの全結合層は同じもの$(W_{in})$を利用し<br>\n",
    "中間層から出力層への全結合層は別のもの$(W_{out})$を利用する。<br>\n",
    "ここでは入力層が２つあるが一般にコンテキストとしてN個の単語がある場合<br>\n",
    "入力層はN個存在する。（one-hotの性質）<br>\n",
    "\n",
    "中間層にあるニューロンは各入力層の<br>\n",
    "全結合による変換の値が「平均」されたものになる。<br>\n",
    "１つ目の入力層が$h_{1}$、２つ目の入力層が$h_{2}$に変換されたとすると<br>\n",
    "中間値のニューロンは$\\frac{1}{2}(h_{1}+h_{2})$になる。<br>\n",
    "\n",
    "出力層は７個のニューロンがあり、ニューロンは各単語に対応している。<br>\n",
    "此のニューロンは各単語の**スコア**であり<br>\n",
    "値が高いほど対応する単語の出現確率も高くなる。<br>\n",
    "ただし、スコアは確率として解釈される前の値であり<br>\n",
    "此のスコアに対してSoftMax関数を適用することで確率を得る。\n",
    "\n",
    "入力層から中間層までの全結合層は$(W_{in})$で行われる。<br>\n",
    "此のときの重み$W_{in}$こそが単語の分散表現になる。<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f201a333-efbc-4e11-a7cb-83b919e7aea0",
   "metadata": {},
   "source": [
    "<img src=\"zerotuku2-3-10.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ee8c70-0fb7-4873-ad16-d7ba4cadc0b0",
   "metadata": {},
   "source": [
    "上の図のように、重み$W_{in}$の各行には<br>\n",
    "それぞれの単語の分散表現が格納されていると考える。<br>\n",
    "そして学習を重ねることで、コンテキストから出現する単語をうまく推測できるように<br>\n",
    "各単語の分散表現が更新されていく。<br>\n",
    "此のようにして得られたベクトルには「単語の意味」もエンコードされていく。<br>\n",
    "これがword2vecの全体像になる。<br>\n",
    "\n",
    "ここからCBOWモデルを「ニューロン視点」でなく「レイヤ視点」で図示する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d2ef79-e754-453e-9dcb-00e212307c18",
   "metadata": {},
   "source": [
    "<img src=\"zerotuku2-3-11.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d150d3-6219-4f57-b187-e0f9e5f266aa",
   "metadata": {},
   "source": [
    "図のようにCBOWモデルは最初に２つのMatMulレイヤがあり<br>\n",
    "その２つの出力が互いに加算される。<br>\n",
    "その加算された値に0.5をかけることで平均を求め、それが中間層となる。<br>\n",
    "最後にその中間層に対して別のMatMulレイヤが適用され「スコア」が出力。\n",
    "\n",
    "此の図を参考に実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2c61d55-091b-43f8-ab33-eced4bce081e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.75677671 -0.13899965  0.19110736 -1.02533292 -0.64859123  0.48203161\n",
      "  -0.11932619]]\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from layers import MatMul\n",
    "\n",
    "\n",
    "# サンプルのコンテキストデータ\n",
    "c0 = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
    "c1 = np.array([[0, 0, 1, 0, 0, 0, 0]])\n",
    "\n",
    "# 重みの初期化\n",
    "W_in = np.random.randn(7, 3)\n",
    "W_out = np.random.randn(3, 7)\n",
    "\n",
    "# レイヤの生成\n",
    "in_layer0 = MatMul(W_in)\n",
    "in_layer1 = MatMul(W_in)\n",
    "out_layer = MatMul(W_out)\n",
    "\n",
    "# 順伝搬\n",
    "h0 = in_layer0.forward(c0)\n",
    "h1 = in_layer1.forward(c1)\n",
    "h = 0.5 * (h0 + h1)\n",
    "s = out_layer.forward(h)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ac00b79-0d53-4369-9a21-88dae4dae1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.0942075   1.0260416   0.79625547  1.7419446   0.8473527   0.24108122\n",
      "  -0.8834031 ]]\n"
     ]
    }
   ],
   "source": [
    "#tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# コンテキスト\n",
    "c0 = tf.constant([[1, 0, 0, 0, 0, 0, 0]], dtype=tf.float32)\n",
    "c1 = tf.constant([[0, 0, 1, 0, 0, 0, 0]], dtype=tf.float32)\n",
    "\n",
    "# 重み\n",
    "W_in = tf.random.normal((7, 3))\n",
    "W_out = tf.random.normal((3, 7))\n",
    "\n",
    "# レイヤー\n",
    "in_layer0 = MatMulTF(W_in)\n",
    "in_layer1 = MatMulTF(W_in)   # 同じ重みを共有する場合は同じインスタンスでもOK\n",
    "out_layer = MatMulTF(W_out)\n",
    "\n",
    "# 順伝搬\n",
    "h0 = in_layer0.forward(c0)\n",
    "h1 = in_layer1.forward(c1)\n",
    "h = 0.5 * (h0 + h1)\n",
    "s = out_layer.forward(h)\n",
    "\n",
    "print(s.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff894aef-35de-441f-bada-e320188fbd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.3698,  0.7803,  0.8962, -0.7728, -0.6296,  0.2652,  0.3762]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#pytorch\n",
    "import torch\n",
    "\n",
    "class MatMulTorch:\n",
    "    def __init__(self, W):\n",
    "        self.W = torch.nn.Parameter(W)  # 学習対象なら Parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.matmul(x, self.W)\n",
    "\n",
    "# コンテキスト\n",
    "c0 = torch.tensor([[1, 0, 0, 0, 0, 0, 0]], dtype=torch.float32)\n",
    "c1 = torch.tensor([[0, 0, 1, 0, 0, 0, 0]], dtype=torch.float32)\n",
    "\n",
    "# 重み\n",
    "W_in = torch.randn(7, 3)\n",
    "W_out = torch.randn(3, 7)\n",
    "\n",
    "# レイヤー\n",
    "in_layer0 = MatMulTorch(W_in)\n",
    "in_layer1 = MatMulTorch(W_in)\n",
    "out_layer = MatMulTorch(W_out)\n",
    "\n",
    "# 順伝搬\n",
    "h0 = in_layer0.forward(c0)\n",
    "h1 = in_layer1.forward(c1)\n",
    "h = 0.5 * (h0 + h1)\n",
    "s = out_layer.forward(h)\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05f4a23-5246-4126-8a38-cc022fd6e78f",
   "metadata": {},
   "source": [
    "ここでは最初に、必要な重みを初期化。<br>\n",
    "入力層を処理するMatMulレイヤをコンテキストの数だけ生成。<br>\n",
    "出力層側のレイヤを一つだけ生成。<br>\n",
    "このとき、入力層側のMatMulレイヤは重み$W_{in}$を共有する点に注意。<br>\n",
    "\n",
    "あとは入力層側のメソッドを呼び、forward()メソッドを呼び、中間データを計算し<br>\n",
    "出力層側のMatMulレイヤによって各単語のスコアを求める。<br>\n",
    "\n",
    "以上がCBOWの推論処理。此のモデルは活性化関数を使わないシンプルなネットワーク構成。<br>\n",
    "入力層が複数個あり、そこでの重みを共有することを除けば難しい点はないであろう。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93543a7e-f1c8-45ae-bae6-d556ada690ce",
   "metadata": {},
   "source": [
    "## 3.2.2 CBOWモデルの学習\n",
    "\n",
    "ここまでで出力層において各単語のスコアを出力した。<br>\n",
    "これから得られたスコアに対してSoftMax関数を使うことで「確率」を得る。<br>\n",
    "此の確率はコンテキストが与えられたときにその中央に度の単語が出力するかを表す。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135cffef-fad8-47eb-b6d9-2273c152107f",
   "metadata": {},
   "source": [
    "<img src=\"zerotuku2-3-12.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56001f7f-2086-41f0-91dd-acdbb3612aff",
   "metadata": {},
   "source": [
    "この例は「you」「goodbye」がコンテキストで正解ラベルが「say」の場合である。<br>\n",
    "良い重みのネットワークがあれば<br>\n",
    "正解に対するニューロンが高くなっていることに期待が持てる。<br>\n",
    "\n",
    "ニューラルネットワークの学習について考える。<br>\n",
    "ここでは多クラス分類を行うニューラルネットワークに対応した<br>\n",
    "SoftMaxと交差エントロピー誤差を求め、それを損失として学習を進める。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e19285-2a5e-4c67-8f79-a7520bf840d4",
   "metadata": {},
   "source": [
    "<img src=\"zerotuku2-3-13.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7197edb3-1b9b-41fb-8ee4-1377770080ab",
   "metadata": {},
   "source": [
    "## 3.2.3 word2vecの重みと分散表現\n",
    "\n",
    "これまでの説明通り、word2vecのネットワークには２つの重みがある。<br>\n",
    "$W_{in}$は話したとおり各単語の分散表現に対応する。<br>\n",
    "そのように考えると$W_{out}$にも単語の意味がエンコードされたベクトルが<br>\n",
    "格納されていると考えることができる<br>\n",
    "ただし、出力側の重みは列方向の単語の分散表現が格納されている。<br>\n",
    "しかし、word2vecでは基本的には入力側の重みを利用することが多い。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24204ccb-692a-433e-85d0-f84bcabaa27b",
   "metadata": {},
   "source": [
    "# 3.3 学習データの準備\n",
    "\n",
    "これから学習を行うにあたってコーパスは今までと同様のものを使う。\n",
    "\n",
    "## 3.3.1 コンテキストとターゲット\n",
    "\n",
    "ここからは正解ラベルを「ターゲット」、入力を「コンテキスト」と呼ぶ。\n",
    "\n",
    "コーパスの中から対象とする単語を「ターゲット」\n",
    "その周囲の単語を「コーパス」として抜き出す。\n",
    "\n",
    "これらを作成する関数を実装する。\n",
    "まずはコーパスのテキストを単語IDに変換する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "090c8714-d05a-48e7-b166-49b33e79308f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.util import preprocess\n",
    "\n",
    "text='You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word=preprocess(text)\n",
    "print(corpus)\n",
    "\n",
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c313adf4-b218-43a4-a436-4b902af22e9b",
   "metadata": {},
   "source": [
    "corpusを与えるとcontextsとtargetを返すような関数を実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbbea3a7-279d-4bf2-adce-031d1fd01ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2, 6)\n",
      "(5, 6)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.util import preprocess,create_contexts_target,convert_one_hot\n",
    "\n",
    "text='You say goodbye and I say hello'\n",
    "corpus, word_to_id, id_to_word=preprocess(text)\n",
    "\n",
    "contexts,target=create_contexts_target(corpus,window_size=1)\n",
    "\n",
    "vocab_size=len(word_to_id)\n",
    "target=convert_one_hot(target,vocab_size)\n",
    "contexts=convert_one_hot(contexts,vocab_size)\n",
    "\n",
    "print(contexts.shape)\n",
    "\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "094a6e88-6ece-4ebe-b982-5846abde486f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contexts_one_hot shape: (5, 2, 6)\n",
      "target_one_hot shape: (5, 6)\n",
      "id_to_word: {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello'}\n"
     ]
    }
   ],
   "source": [
    "#tensorflow\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.util import preprocess, create_contexts_target\n",
    "import tensorflow as tf\n",
    "\n",
    "# テキスト前処理\n",
    "text = 'You say goodbye and I say hello'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "# コンテキストとターゲット作成\n",
    "contexts, target = create_contexts_target(corpus, window_size=1)\n",
    "\n",
    "# 語彙数\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "# NumPy → Tensor にする\n",
    "contexts_tf = tf.constant(contexts, dtype=tf.int32)   # shape: (N, window_size*2)\n",
    "target_tf = tf.constant(target, dtype=tf.int32)       # shape: (N,)\n",
    "\n",
    "# One-hot に変換\n",
    "contexts_one_hot = tf.one_hot(contexts_tf, vocab_size) # shape: (N, window_size*2, vocab_size)\n",
    "target_one_hot = tf.one_hot(target_tf, vocab_size)     # shape: (N, vocab_size)\n",
    "\n",
    "print('contexts_one_hot shape:', contexts_one_hot.shape)\n",
    "print('target_one_hot shape:', target_one_hot.shape)\n",
    "print('id_to_word:', id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dafcd604-177d-4396-84d7-48e558345b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contexts_one_hot shape: torch.Size([5, 2, 6])\n",
      "target_one_hot shape: torch.Size([5, 6])\n",
      "id_to_word: {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello'}\n"
     ]
    }
   ],
   "source": [
    "#pytorch\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.util import preprocess, create_contexts_target\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# テキスト前処理\n",
    "text = 'You say goodbye and I say hello'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size=1)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "# Tensor にする\n",
    "contexts_torch = torch.tensor(contexts, dtype=torch.long)\n",
    "target_torch = torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "# One-hot にする\n",
    "contexts_one_hot = F.one_hot(contexts_torch, num_classes=vocab_size)\n",
    "target_one_hot = F.one_hot(target_torch, num_classes=vocab_size)\n",
    "\n",
    "print('contexts_one_hot shape:', contexts_one_hot.shape)\n",
    "print('target_one_hot shape:', target_one_hot.shape)\n",
    "print('id_to_word:', id_to_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb61d80-06b6-451c-87b1-166649c80723",
   "metadata": {},
   "source": [
    "# 3.4 CBOWモデルの実装\n",
    "\n",
    "ニューラルネットワークの名前をSimpleCBOWとして実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e752bb4e-01a8-4588-85f3-801afdc59ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.layers import MatMul, SoftmaxWithLoss\n",
    "\n",
    "\n",
    "class SimpleCBOW:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        # 重みの初期化\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.in_layer0 = MatMul(W_in)\n",
    "        self.in_layer1 = MatMul(W_in)\n",
    "        self.out_layer = MatMul(W_out)\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "\n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # メンバ変数に単語の分散表現を設定\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h0 = self.in_layer0.forward(contexts[:, 0])\n",
    "        h1 = self.in_layer1.forward(contexts[:, 1])\n",
    "        h = (h0 + h1) * 0.5\n",
    "        score = self.out_layer.forward(h)\n",
    "        loss = self.loss_layer.forward(score, target)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        ds = self.loss_layer.backward(dout)\n",
    "        da = self.out_layer.backward(ds)\n",
    "        da *= 0.5\n",
    "        self.in_layer1.backward(da)\n",
    "        self.in_layer0.backward(da)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74be036e-39ee-45d5-bb60-774afae389cf",
   "metadata": {},
   "source": [
    "ここではイニシャライザの引数として語彙数と中間層のニューロン数を取る。<br>\n",
    "重みの初期化は２つ生成、これは小さなランダム値によって初期化。<br>\n",
    "このときのNumpy配列のデータ型をastype('f')で指定。→　32ビットで初期化<br>\n",
    "\n",
    "続いて必要なレイヤの生成。$W_{in}$,$W_{out}$,SoftMaxを生成。<br>\n",
    "ここで、出力側のMatMulレイヤはコンテキストで使用する単語数だけ作る。<br>\n",
    "そしてMatMulを初期化。\n",
    "\n",
    "最後にニューラルネットワークで使われるパラメータと勾配を<br>\n",
    "paramsとgradsに配列としてまとめる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58e3921-2b74-434f-84cd-2e576dd37f7b",
   "metadata": {},
   "source": [
    "## 3.4.1 学習コードの実装\n",
    "\n",
    "通常のニューラルネットワークと同じように学習させる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7362a84e-03c9-4eb8-8f99-a80171c3ea3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 2 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 3 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 4 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 5 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 6 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 7 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 8 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 9 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 10 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 11 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 12 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 13 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 14 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 15 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 16 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 17 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 18 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 19 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 20 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 21 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 22 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 23 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 24 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 25 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 26 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 27 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 28 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 29 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 30 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 31 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 32 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 33 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 34 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 35 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 36 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
      "| epoch 37 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
      "| epoch 38 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
      "| epoch 39 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
      "| epoch 40 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
      "| epoch 41 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
      "| epoch 42 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
      "| epoch 43 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
      "| epoch 44 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
      "| epoch 45 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
      "| epoch 46 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
      "| epoch 47 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
      "| epoch 48 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
      "| epoch 49 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
      "| epoch 50 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
      "| epoch 51 |  iter 1 / 2 | time 0[s] | loss 1.89\n",
      "| epoch 52 |  iter 1 / 2 | time 0[s] | loss 1.89\n",
      "| epoch 53 |  iter 1 / 2 | time 0[s] | loss 1.89\n",
      "| epoch 54 |  iter 1 / 2 | time 0[s] | loss 1.89\n",
      "| epoch 55 |  iter 1 / 2 | time 0[s] | loss 1.88\n",
      "| epoch 56 |  iter 1 / 2 | time 0[s] | loss 1.88\n",
      "| epoch 57 |  iter 1 / 2 | time 0[s] | loss 1.88\n",
      "| epoch 58 |  iter 1 / 2 | time 0[s] | loss 1.88\n",
      "| epoch 59 |  iter 1 / 2 | time 0[s] | loss 1.87\n",
      "| epoch 60 |  iter 1 / 2 | time 0[s] | loss 1.87\n",
      "| epoch 61 |  iter 1 / 2 | time 0[s] | loss 1.87\n",
      "| epoch 62 |  iter 1 / 2 | time 0[s] | loss 1.86\n",
      "| epoch 63 |  iter 1 / 2 | time 0[s] | loss 1.86\n",
      "| epoch 64 |  iter 1 / 2 | time 0[s] | loss 1.87\n",
      "| epoch 65 |  iter 1 / 2 | time 0[s] | loss 1.85\n",
      "| epoch 66 |  iter 1 / 2 | time 0[s] | loss 1.86\n",
      "| epoch 67 |  iter 1 / 2 | time 0[s] | loss 1.85\n",
      "| epoch 68 |  iter 1 / 2 | time 0[s] | loss 1.85\n",
      "| epoch 69 |  iter 1 / 2 | time 0[s] | loss 1.84\n",
      "| epoch 70 |  iter 1 / 2 | time 0[s] | loss 1.85\n",
      "| epoch 71 |  iter 1 / 2 | time 0[s] | loss 1.83\n",
      "| epoch 72 |  iter 1 / 2 | time 0[s] | loss 1.84\n",
      "| epoch 73 |  iter 1 / 2 | time 0[s] | loss 1.83\n",
      "| epoch 74 |  iter 1 / 2 | time 0[s] | loss 1.82\n",
      "| epoch 75 |  iter 1 / 2 | time 0[s] | loss 1.82\n",
      "| epoch 76 |  iter 1 / 2 | time 0[s] | loss 1.83\n",
      "| epoch 77 |  iter 1 / 2 | time 0[s] | loss 1.80\n",
      "| epoch 78 |  iter 1 / 2 | time 0[s] | loss 1.81\n",
      "| epoch 79 |  iter 1 / 2 | time 0[s] | loss 1.80\n",
      "| epoch 80 |  iter 1 / 2 | time 0[s] | loss 1.81\n",
      "| epoch 81 |  iter 1 / 2 | time 0[s] | loss 1.80\n",
      "| epoch 82 |  iter 1 / 2 | time 0[s] | loss 1.79\n",
      "| epoch 83 |  iter 1 / 2 | time 0[s] | loss 1.79\n",
      "| epoch 84 |  iter 1 / 2 | time 0[s] | loss 1.79\n",
      "| epoch 85 |  iter 1 / 2 | time 0[s] | loss 1.79\n",
      "| epoch 86 |  iter 1 / 2 | time 0[s] | loss 1.78\n",
      "| epoch 87 |  iter 1 / 2 | time 0[s] | loss 1.78\n",
      "| epoch 88 |  iter 1 / 2 | time 0[s] | loss 1.75\n",
      "| epoch 89 |  iter 1 / 2 | time 0[s] | loss 1.77\n",
      "| epoch 90 |  iter 1 / 2 | time 0[s] | loss 1.76\n",
      "| epoch 91 |  iter 1 / 2 | time 0[s] | loss 1.76\n",
      "| epoch 92 |  iter 1 / 2 | time 0[s] | loss 1.75\n",
      "| epoch 93 |  iter 1 / 2 | time 0[s] | loss 1.75\n",
      "| epoch 94 |  iter 1 / 2 | time 0[s] | loss 1.74\n",
      "| epoch 95 |  iter 1 / 2 | time 0[s] | loss 1.74\n",
      "| epoch 96 |  iter 1 / 2 | time 0[s] | loss 1.74\n",
      "| epoch 97 |  iter 1 / 2 | time 0[s] | loss 1.74\n",
      "| epoch 98 |  iter 1 / 2 | time 0[s] | loss 1.72\n",
      "| epoch 99 |  iter 1 / 2 | time 0[s] | loss 1.72\n",
      "| epoch 100 |  iter 1 / 2 | time 0[s] | loss 1.72\n",
      "| epoch 101 |  iter 1 / 2 | time 0[s] | loss 1.69\n",
      "| epoch 102 |  iter 1 / 2 | time 0[s] | loss 1.71\n",
      "| epoch 103 |  iter 1 / 2 | time 0[s] | loss 1.70\n",
      "| epoch 104 |  iter 1 / 2 | time 0[s] | loss 1.70\n",
      "| epoch 105 |  iter 1 / 2 | time 0[s] | loss 1.70\n",
      "| epoch 106 |  iter 1 / 2 | time 0[s] | loss 1.67\n",
      "| epoch 107 |  iter 1 / 2 | time 0[s] | loss 1.69\n",
      "| epoch 108 |  iter 1 / 2 | time 0[s] | loss 1.68\n",
      "| epoch 109 |  iter 1 / 2 | time 0[s] | loss 1.67\n",
      "| epoch 110 |  iter 1 / 2 | time 0[s] | loss 1.68\n",
      "| epoch 111 |  iter 1 / 2 | time 0[s] | loss 1.64\n",
      "| epoch 112 |  iter 1 / 2 | time 0[s] | loss 1.68\n",
      "| epoch 113 |  iter 1 / 2 | time 0[s] | loss 1.65\n",
      "| epoch 114 |  iter 1 / 2 | time 0[s] | loss 1.65\n",
      "| epoch 115 |  iter 1 / 2 | time 0[s] | loss 1.64\n",
      "| epoch 116 |  iter 1 / 2 | time 0[s] | loss 1.63\n",
      "| epoch 117 |  iter 1 / 2 | time 0[s] | loss 1.64\n",
      "| epoch 118 |  iter 1 / 2 | time 0[s] | loss 1.64\n",
      "| epoch 119 |  iter 1 / 2 | time 0[s] | loss 1.64\n",
      "| epoch 120 |  iter 1 / 2 | time 0[s] | loss 1.60\n",
      "| epoch 121 |  iter 1 / 2 | time 0[s] | loss 1.65\n",
      "| epoch 122 |  iter 1 / 2 | time 0[s] | loss 1.58\n",
      "| epoch 123 |  iter 1 / 2 | time 0[s] | loss 1.62\n",
      "| epoch 124 |  iter 1 / 2 | time 0[s] | loss 1.63\n",
      "| epoch 125 |  iter 1 / 2 | time 0[s] | loss 1.57\n",
      "| epoch 126 |  iter 1 / 2 | time 0[s] | loss 1.60\n",
      "| epoch 127 |  iter 1 / 2 | time 0[s] | loss 1.57\n",
      "| epoch 128 |  iter 1 / 2 | time 0[s] | loss 1.59\n",
      "| epoch 129 |  iter 1 / 2 | time 0[s] | loss 1.58\n",
      "| epoch 130 |  iter 1 / 2 | time 0[s] | loss 1.56\n",
      "| epoch 131 |  iter 1 / 2 | time 0[s] | loss 1.59\n",
      "| epoch 132 |  iter 1 / 2 | time 0[s] | loss 1.59\n",
      "| epoch 133 |  iter 1 / 2 | time 0[s] | loss 1.53\n",
      "| epoch 134 |  iter 1 / 2 | time 0[s] | loss 1.59\n",
      "| epoch 135 |  iter 1 / 2 | time 0[s] | loss 1.51\n",
      "| epoch 136 |  iter 1 / 2 | time 0[s] | loss 1.52\n",
      "| epoch 137 |  iter 1 / 2 | time 0[s] | loss 1.60\n",
      "| epoch 138 |  iter 1 / 2 | time 0[s] | loss 1.53\n",
      "| epoch 139 |  iter 1 / 2 | time 0[s] | loss 1.51\n",
      "| epoch 140 |  iter 1 / 2 | time 0[s] | loss 1.54\n",
      "| epoch 141 |  iter 1 / 2 | time 0[s] | loss 1.49\n",
      "| epoch 142 |  iter 1 / 2 | time 0[s] | loss 1.52\n",
      "| epoch 143 |  iter 1 / 2 | time 0[s] | loss 1.48\n",
      "| epoch 144 |  iter 1 / 2 | time 0[s] | loss 1.56\n",
      "| epoch 145 |  iter 1 / 2 | time 0[s] | loss 1.47\n",
      "| epoch 146 |  iter 1 / 2 | time 0[s] | loss 1.49\n",
      "| epoch 147 |  iter 1 / 2 | time 0[s] | loss 1.47\n",
      "| epoch 148 |  iter 1 / 2 | time 0[s] | loss 1.49\n",
      "| epoch 149 |  iter 1 / 2 | time 0[s] | loss 1.50\n",
      "| epoch 150 |  iter 1 / 2 | time 0[s] | loss 1.45\n",
      "| epoch 151 |  iter 1 / 2 | time 0[s] | loss 1.51\n",
      "| epoch 152 |  iter 1 / 2 | time 0[s] | loss 1.47\n",
      "| epoch 153 |  iter 1 / 2 | time 0[s] | loss 1.42\n",
      "| epoch 154 |  iter 1 / 2 | time 0[s] | loss 1.46\n",
      "| epoch 155 |  iter 1 / 2 | time 0[s] | loss 1.47\n",
      "| epoch 156 |  iter 1 / 2 | time 0[s] | loss 1.47\n",
      "| epoch 157 |  iter 1 / 2 | time 0[s] | loss 1.44\n",
      "| epoch 158 |  iter 1 / 2 | time 0[s] | loss 1.43\n",
      "| epoch 159 |  iter 1 / 2 | time 0[s] | loss 1.43\n",
      "| epoch 160 |  iter 1 / 2 | time 0[s] | loss 1.44\n",
      "| epoch 161 |  iter 1 / 2 | time 0[s] | loss 1.41\n",
      "| epoch 162 |  iter 1 / 2 | time 0[s] | loss 1.42\n",
      "| epoch 163 |  iter 1 / 2 | time 0[s] | loss 1.43\n",
      "| epoch 164 |  iter 1 / 2 | time 0[s] | loss 1.36\n",
      "| epoch 165 |  iter 1 / 2 | time 0[s] | loss 1.45\n",
      "| epoch 166 |  iter 1 / 2 | time 0[s] | loss 1.43\n",
      "| epoch 167 |  iter 1 / 2 | time 0[s] | loss 1.40\n",
      "| epoch 168 |  iter 1 / 2 | time 0[s] | loss 1.37\n",
      "| epoch 169 |  iter 1 / 2 | time 0[s] | loss 1.34\n",
      "| epoch 170 |  iter 1 / 2 | time 0[s] | loss 1.41\n",
      "| epoch 171 |  iter 1 / 2 | time 0[s] | loss 1.43\n",
      "| epoch 172 |  iter 1 / 2 | time 0[s] | loss 1.28\n",
      "| epoch 173 |  iter 1 / 2 | time 0[s] | loss 1.46\n",
      "| epoch 174 |  iter 1 / 2 | time 0[s] | loss 1.37\n",
      "| epoch 175 |  iter 1 / 2 | time 0[s] | loss 1.31\n",
      "| epoch 176 |  iter 1 / 2 | time 0[s] | loss 1.37\n",
      "| epoch 177 |  iter 1 / 2 | time 0[s] | loss 1.32\n",
      "| epoch 178 |  iter 1 / 2 | time 0[s] | loss 1.43\n",
      "| epoch 179 |  iter 1 / 2 | time 0[s] | loss 1.31\n",
      "| epoch 180 |  iter 1 / 2 | time 0[s] | loss 1.38\n",
      "| epoch 181 |  iter 1 / 2 | time 0[s] | loss 1.31\n",
      "| epoch 182 |  iter 1 / 2 | time 0[s] | loss 1.28\n",
      "| epoch 183 |  iter 1 / 2 | time 0[s] | loss 1.43\n",
      "| epoch 184 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
      "| epoch 185 |  iter 1 / 2 | time 0[s] | loss 1.32\n",
      "| epoch 186 |  iter 1 / 2 | time 0[s] | loss 1.32\n",
      "| epoch 187 |  iter 1 / 2 | time 0[s] | loss 1.33\n",
      "| epoch 188 |  iter 1 / 2 | time 0[s] | loss 1.26\n",
      "| epoch 189 |  iter 1 / 2 | time 0[s] | loss 1.36\n",
      "| epoch 190 |  iter 1 / 2 | time 0[s] | loss 1.31\n",
      "| epoch 191 |  iter 1 / 2 | time 0[s] | loss 1.28\n",
      "| epoch 192 |  iter 1 / 2 | time 0[s] | loss 1.32\n",
      "| epoch 193 |  iter 1 / 2 | time 0[s] | loss 1.28\n",
      "| epoch 194 |  iter 1 / 2 | time 0[s] | loss 1.29\n",
      "| epoch 195 |  iter 1 / 2 | time 0[s] | loss 1.30\n",
      "| epoch 196 |  iter 1 / 2 | time 0[s] | loss 1.28\n",
      "| epoch 197 |  iter 1 / 2 | time 0[s] | loss 1.28\n",
      "| epoch 198 |  iter 1 / 2 | time 0[s] | loss 1.26\n",
      "| epoch 199 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
      "| epoch 200 |  iter 1 / 2 | time 0[s] | loss 1.34\n",
      "| epoch 201 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
      "| epoch 202 |  iter 1 / 2 | time 0[s] | loss 1.32\n",
      "| epoch 203 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
      "| epoch 204 |  iter 1 / 2 | time 0[s] | loss 1.29\n",
      "| epoch 205 |  iter 1 / 2 | time 0[s] | loss 1.26\n",
      "| epoch 206 |  iter 1 / 2 | time 0[s] | loss 1.31\n",
      "| epoch 207 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
      "| epoch 208 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
      "| epoch 209 |  iter 1 / 2 | time 0[s] | loss 1.30\n",
      "| epoch 210 |  iter 1 / 2 | time 0[s] | loss 1.25\n",
      "| epoch 211 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
      "| epoch 212 |  iter 1 / 2 | time 0[s] | loss 1.21\n",
      "| epoch 213 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
      "| epoch 214 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
      "| epoch 215 |  iter 1 / 2 | time 0[s] | loss 1.36\n",
      "| epoch 216 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
      "| epoch 217 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
      "| epoch 218 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
      "| epoch 219 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
      "| epoch 220 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
      "| epoch 221 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
      "| epoch 222 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
      "| epoch 223 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
      "| epoch 224 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
      "| epoch 225 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
      "| epoch 226 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
      "| epoch 227 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
      "| epoch 228 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
      "| epoch 229 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
      "| epoch 230 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
      "| epoch 231 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
      "| epoch 232 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
      "| epoch 233 |  iter 1 / 2 | time 0[s] | loss 1.33\n",
      "| epoch 234 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 235 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
      "| epoch 236 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
      "| epoch 237 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
      "| epoch 238 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
      "| epoch 239 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
      "| epoch 240 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
      "| epoch 241 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
      "| epoch 242 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
      "| epoch 243 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
      "| epoch 244 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
      "| epoch 245 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
      "| epoch 246 |  iter 1 / 2 | time 0[s] | loss 1.21\n",
      "| epoch 247 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 248 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
      "| epoch 249 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
      "| epoch 250 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 251 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
      "| epoch 252 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 253 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 254 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 255 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
      "| epoch 256 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
      "| epoch 257 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
      "| epoch 258 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
      "| epoch 259 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
      "| epoch 260 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 261 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
      "| epoch 262 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
      "| epoch 263 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
      "| epoch 264 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 265 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
      "| epoch 266 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
      "| epoch 267 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 268 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
      "| epoch 269 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 270 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
      "| epoch 271 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
      "| epoch 272 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
      "| epoch 273 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 274 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
      "| epoch 275 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 276 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
      "| epoch 277 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
      "| epoch 278 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 279 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
      "| epoch 280 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
      "| epoch 281 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
      "| epoch 282 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 283 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
      "| epoch 284 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 285 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
      "| epoch 286 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 287 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 288 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
      "| epoch 289 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
      "| epoch 290 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 291 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
      "| epoch 292 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 293 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 294 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
      "| epoch 295 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 296 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
      "| epoch 297 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 298 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 299 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 300 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 301 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 302 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 303 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 304 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 305 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
      "| epoch 306 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 307 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
      "| epoch 308 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
      "| epoch 309 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 310 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 311 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 312 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 313 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 314 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 315 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 316 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 317 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 318 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
      "| epoch 319 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 320 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 321 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 322 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 323 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 324 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 325 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 326 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 327 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 328 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 329 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 330 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 331 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 332 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
      "| epoch 333 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 334 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 335 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
      "| epoch 336 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 337 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 338 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 339 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 340 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
      "| epoch 341 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 342 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 343 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 344 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 345 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 346 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
      "| epoch 347 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 348 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 349 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 350 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 351 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 352 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
      "| epoch 353 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 354 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 355 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 356 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 357 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
      "| epoch 358 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 359 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 360 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 361 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 362 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
      "| epoch 363 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 364 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 365 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 366 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 367 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
      "| epoch 368 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 369 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
      "| epoch 370 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 371 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 372 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 373 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 374 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 375 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
      "| epoch 376 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 377 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 378 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 379 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
      "| epoch 380 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 381 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
      "| epoch 382 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
      "| epoch 383 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 384 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
      "| epoch 385 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 386 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 387 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 388 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 389 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 390 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 391 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
      "| epoch 392 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 393 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 394 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
      "| epoch 395 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 396 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 397 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 398 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
      "| epoch 399 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 400 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 401 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 402 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 403 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 404 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
      "| epoch 405 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 406 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
      "| epoch 407 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 408 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 409 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
      "| epoch 410 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 411 |  iter 1 / 2 | time 0[s] | loss 1.21\n",
      "| epoch 412 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 413 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 414 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 415 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 416 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 417 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
      "| epoch 418 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 419 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 420 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
      "| epoch 421 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 422 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
      "| epoch 423 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 424 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 425 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 426 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 427 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 428 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 429 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 430 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 431 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
      "| epoch 432 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 433 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 434 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 435 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 436 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 437 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
      "| epoch 438 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 439 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 440 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 441 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 442 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 443 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 444 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 445 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
      "| epoch 446 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 447 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 448 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 449 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 450 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 451 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 452 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
      "| epoch 453 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 454 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
      "| epoch 455 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 456 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 457 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 458 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 459 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 460 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 461 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 462 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 463 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
      "| epoch 464 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 465 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 466 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 467 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 468 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 469 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 470 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 471 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 472 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 473 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 474 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 475 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
      "| epoch 476 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 477 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 478 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 479 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 480 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
      "| epoch 481 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 482 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 483 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 484 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 485 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
      "| epoch 486 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 487 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 488 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 489 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
      "| epoch 490 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 491 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 492 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 493 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 494 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 495 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 496 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 497 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 498 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 499 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 500 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 501 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 502 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 503 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 504 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 505 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 506 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 507 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 508 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 509 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 510 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 511 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 512 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 513 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
      "| epoch 514 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 515 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 516 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 517 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 518 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 519 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 520 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 521 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 522 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 523 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 524 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 525 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 526 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 527 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 528 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 529 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 530 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 531 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 532 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 533 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 534 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 535 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 536 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
      "| epoch 537 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 538 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 539 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 540 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 541 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
      "| epoch 542 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 543 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 544 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 545 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 546 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 547 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 548 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 549 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 550 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 551 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 552 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 553 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 554 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 555 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 556 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 557 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 558 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 559 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 560 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 561 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 562 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 563 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 564 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 565 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 566 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 567 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 568 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 569 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 570 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 571 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 572 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 573 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 574 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 575 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 576 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 577 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 578 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 579 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 580 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 581 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 582 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 583 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 584 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 585 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 586 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 587 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 588 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 589 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 590 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 591 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 592 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 593 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 594 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 595 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 596 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 597 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 598 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 599 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 600 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 601 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 602 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 603 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 604 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 605 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 606 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 607 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
      "| epoch 608 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 609 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 610 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 611 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 612 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 613 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 614 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 615 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 616 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 617 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 618 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 619 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 620 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 621 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 622 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 623 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 624 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 625 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 626 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 627 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 628 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 629 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 630 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 631 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 632 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 633 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 634 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 635 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 636 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 637 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 638 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 639 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 640 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 641 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 642 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 643 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 644 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 645 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 646 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 647 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 648 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 649 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 650 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 651 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 652 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 653 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 654 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 655 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 656 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 657 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 658 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 659 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 660 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 661 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 662 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 663 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 664 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 665 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 666 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 667 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 668 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 669 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 670 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 671 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 672 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 673 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 674 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
      "| epoch 675 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 676 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 677 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 678 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 679 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 680 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 681 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 682 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 683 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 684 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
      "| epoch 685 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 686 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 687 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 688 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 689 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 690 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 691 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 692 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 693 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 694 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 695 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 696 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 697 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 698 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 699 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 700 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 701 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 702 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 703 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 704 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 705 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 706 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 707 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 708 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 709 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 710 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 711 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 712 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 713 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 714 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 715 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 716 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 717 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 718 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 719 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 720 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 721 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 722 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 723 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 724 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 725 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 726 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 727 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 728 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 729 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 730 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 731 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 732 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 733 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 734 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 735 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 736 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 737 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 738 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 739 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 740 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
      "| epoch 741 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 742 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 743 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 744 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 745 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 746 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 747 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 748 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 749 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 750 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 751 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 752 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 753 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 754 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 755 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 756 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 757 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 758 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 759 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 760 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 761 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 762 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 763 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 764 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 765 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 766 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
      "| epoch 767 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 768 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 769 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 770 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 771 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 772 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 773 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 774 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 775 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
      "| epoch 776 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 777 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 778 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 779 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 780 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 781 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 782 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 783 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 784 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 785 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 786 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 787 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 788 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
      "| epoch 789 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 790 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 791 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 792 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 793 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 794 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 795 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 796 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 797 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 798 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 799 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 800 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 801 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 802 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 803 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 804 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 805 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 806 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 807 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 808 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 809 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 810 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 811 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 812 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 813 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 814 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 815 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 816 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 817 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 818 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 819 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 820 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 821 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 822 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 823 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 824 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 825 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 826 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 827 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 828 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 829 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 830 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 831 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 832 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 833 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 834 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 835 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 836 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 837 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 838 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 839 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 840 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 841 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 842 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 843 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 844 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 845 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 846 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 847 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 848 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 849 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 850 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 851 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 852 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 853 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 854 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 855 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 856 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 857 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 858 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 859 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 860 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 861 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 862 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
      "| epoch 863 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 864 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 865 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 866 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 867 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 868 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
      "| epoch 869 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 870 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 871 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 872 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 873 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 874 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 875 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 876 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 877 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 878 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 879 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 880 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 881 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 882 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 883 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 884 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 885 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 886 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 887 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 888 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 889 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 890 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 891 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 892 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 893 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 894 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 895 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 896 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 897 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 898 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 899 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 900 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 901 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 902 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 903 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 904 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 905 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 906 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 907 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 908 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 909 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 910 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 911 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 912 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 913 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 914 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 915 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 916 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 917 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 918 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 919 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 920 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 921 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 922 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 923 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 924 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 925 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 926 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
      "| epoch 927 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 928 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 929 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 930 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 931 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 932 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 933 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 934 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 935 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 936 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 937 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 938 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 939 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
      "| epoch 940 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 941 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 942 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 943 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 944 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 945 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 946 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 947 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 948 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 949 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 950 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 951 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 952 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 953 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 954 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 955 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 956 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 957 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 958 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 959 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 960 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 961 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 962 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 963 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 964 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 965 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 966 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 967 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 968 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 969 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 970 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 971 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 972 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 973 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 974 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 975 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 976 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 977 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 978 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 979 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 980 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
      "| epoch 981 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 982 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 983 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 984 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 985 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 986 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
      "| epoch 987 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 988 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 989 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 990 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 991 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 992 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 993 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 994 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 995 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 996 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 997 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 998 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 999 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 1000 |  iter 1 / 2 | time 0[s] | loss 0.73\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you [-1.1181744  1.1374502  0.9331635  1.5262753  0.9480893]\n",
      "say [ 1.1094605 -1.0934782 -1.1335863  1.2735984 -1.1110693]\n",
      "goodbye [-0.7325957   0.7743871   0.9848742   0.33548537  0.97482336]\n",
      "and [ 0.8579804 -0.8483737 -0.7167006  2.0062559 -0.7287622]\n",
      "i [-0.7468175   0.775734    0.9766399   0.3529076   0.97119004]\n",
      "hello [-1.1395253  1.1193454  0.9288013  1.523452   0.9609732]\n",
      ". [ 1.1163473  -1.0812362  -1.2409614  -0.60069376 -1.2404716 ]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # 親ディレクトリのファイルをインポートするための設定\n",
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam\n",
    "from common.util import preprocess, create_contexts_target, convert_one_hot\n",
    "\n",
    "\n",
    "window_size = 1\n",
    "hidden_size = 5\n",
    "batch_size = 3\n",
    "max_epoch = 1000\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)\n",
    "\n",
    "model = SimpleCBOW(vocab_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "trainer.plot()\n",
    "\n",
    "word_vecs = model.word_vecs\n",
    "for word_id, word in id_to_word.items():\n",
    "    print(word, word_vecs[word_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3ee5cf-563a-4a80-bf87-f0a9ecf3b4e2",
   "metadata": {},
   "source": [
    "パラメータの更新には、Adamというアルゴリズムを適用。<br>\n",
    "図で示したとおり学習するにつれて損失が減少。<br>\n",
    "ここから学習が終わったパラメータを確認する。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c821089-6ea7-4140-9d8e-99aeab2918c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "class SimpleCBOW(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(SimpleCBOW, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # 埋め込み\n",
    "        self.in_embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=hidden_size)\n",
    "\n",
    "        # 出力重み\n",
    "        self.out_weight = self.add_weight(\n",
    "            shape=(hidden_size, vocab_size),\n",
    "            initializer=tf.keras.initializers.RandomNormal(stddev=0.01),\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        self.loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    def call(self, contexts, target, training=False):\n",
    "        # contexts: shape (batch_size, 2)\n",
    "        h0 = self.in_embedding(contexts[:, 0])  # (batch_size, hidden_size)\n",
    "        h1 = self.in_embedding(contexts[:, 1])\n",
    "        h = 0.5 * (h0 + h1)                      # (batch_size, hidden_size)\n",
    "\n",
    "        score = tf.matmul(h, self.out_weight)   # (batch_size, vocab_size)\n",
    "\n",
    "        loss = self.loss_fn(target, score)      # target: shape (batch_size,)\n",
    "        return loss, score\n",
    "\n",
    "    @property\n",
    "    def word_vecs(self):\n",
    "        return self.in_embedding.weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5b4f704-0897-43be-9879-e088af048b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(SimpleCBOW, self).__init__()\n",
    "        self.in_embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.out_weight = nn.Parameter(torch.randn(hidden_size, vocab_size) * 0.01)\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h0 = self.in_embedding(contexts[:, 0])\n",
    "        h1 = self.in_embedding(contexts[:, 1])\n",
    "        h = 0.5 * (h0 + h1)\n",
    "\n",
    "        score = torch.matmul(h, self.out_weight)  # (batch_size, vocab_size)\n",
    "\n",
    "        loss = F.cross_entropy(score, target)     # target: (batch_size,)\n",
    "        return loss, score\n",
    "\n",
    "    @property\n",
    "    def word_vecs(self):\n",
    "        return self.in_embedding.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95af4a43-c5b1-4c50-b7e1-7fef3d64fd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you [-1.1181744  1.1374502  0.9331635  1.5262753  0.9480893]\n",
      "say [ 1.1094605 -1.0934782 -1.1335863  1.2735984 -1.1110693]\n",
      "goodbye [-0.7325957   0.7743871   0.9848742   0.33548537  0.97482336]\n",
      "and [ 0.8579804 -0.8483737 -0.7167006  2.0062559 -0.7287622]\n",
      "i [-0.7468175   0.775734    0.9766399   0.3529076   0.97119004]\n",
      "hello [-1.1395253  1.1193454  0.9288013  1.523452   0.9609732]\n",
      ". [ 1.1163473  -1.0812362  -1.2409614  -0.60069376 -1.2404716 ]\n"
     ]
    }
   ],
   "source": [
    "word_vecs=model.word_vecs\n",
    "for word_id, word in id_to_word.items():\n",
    "    print(word,word_vecs[word_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e16abf5-76d5-41d6-a3ed-f5ffee942720",
   "metadata": {},
   "source": [
    "## 3.5.1 CBOWモデルと確率\n",
    "\n",
    "これまでword3vecのCBOWモデルについて見てきた。<br>\n",
    "これを確立の観点から見る。<br>\n",
    "同時確率を$P(A,B)$、事後確率を$P(A|B)$とする時<br>\n",
    "CBOWモデルを確率に沿って記載すると<br>\n",
    "$P(w_{t}|w_{t-1},w_{t+1})$となり<br>\n",
    "これは$w_{t-1}$と$w_{t+1}$が起こったときに$w_{t}$が起こる確率と解釈できる。<br>\n",
    "これを用いることで損失関数に対しても\n",
    "$$\n",
    "L=-\\sum_{k}t_{k}logy_{k}=-logP(w_{t}|w_{t-1},w_{t+1})\n",
    "$$\n",
    "とすることができる。（**負の対数尤度**）\n",
    "\n",
    "これをコーパス全体に拡張すると\n",
    "$$\n",
    "L=-\\frac{1}{T}\\sum_{t=1}^{T}logP(w_{t}|w_{t-1},w_{t+1})\n",
    "$$\n",
    "となる。\n",
    "\n",
    "## 3.5.2 skip-gramモデル\n",
    "\n",
    "前にも述べたとおりword2vecでは2つのモデルが提案される。\n",
    "これまでのものとskip-gramモデルである。\n",
    "こちらはCBOWのコンテキストとターゲットを逆転させたモデルである。\n",
    "この場合、ターゲットからコンテキストを推測することとなる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419a4cdc-f9e6-486e-9147-3c20a15bdd8c",
   "metadata": {},
   "source": [
    "<img src=\"zerotuku2-3-24.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cbd940-06a9-4343-b152-bd1cbd53e844",
   "metadata": {},
   "source": [
    "このときのモデルの損失関数は\n",
    "$$\n",
    "L=-\\frac{1}{T}\\sum_{t=1}^{T}(logP(w_{t-1}|w_{t})+logP(w_{t+1}|w_{t}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d370f4-7f37-4587-9fa9-5bd531d5c7ae",
   "metadata": {},
   "source": [
    "此の2つのモデルを比べた時\n",
    "単語の分散表現の精度の点においてskip-gramのほうがより良い結果を求めやすい。\n",
    "ただし、学習の速度としてはCBOWのほうが早い。\n",
    "\n",
    "## 3.5.3 カウントベースvs推論ベース\n",
    "\n",
    "単語の類似性に対する定量的な評価に関しては\n",
    "推論ベースとカウントベースでは優劣がつけられない。\n",
    "\n",
    "しかし語彙を追加するケースではパラメータの再学習ができる点で\n",
    "推論ベースの手法のほうが効率的と言える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f932d5a-9417-484f-abc4-7893c771b77f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
